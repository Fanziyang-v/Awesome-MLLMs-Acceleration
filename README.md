# Awesome-MLLMs-Acceleration
This is a list of some awesome works on accelerating in Multimodal Large Language Models(MLLMs).



## :bar_chart:Benchmarks

1. [MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2306.13394) (Jun. 23, 2023) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.13394)[![github](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models)](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)![alias](https://img.shields.io/badge/MME-black)
2. [MMBench: Is Your Multi-modal Model an All-around Player?](https://arxiv.org/abs/2307.06281) (Jul. 12, 2023, **ECCV 2024**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2307.06281)[![github](https://img.shields.io/github/stars/open-compass/VLMEvalKit)](https://github.com/open-compass/VLMEvalKit)![tag](https://img.shields.io/badge/Oral-FF4D00)![alias](https://img.shields.io/badge/MMBench-black)
3. [Evaluating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2305.10355) (May. 17, 2023, **EMNLP 2023**) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.10355)[![github](https://img.shields.io/github/stars/AoiDragon/POPE)](https://github.com/AoiDragon/POPE)![alias](https://img.shields.io/badge/PoPE-black)
4. [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://arxiv.org/abs/2209.09513) (Sep. 20, 2022, **NeurIPS 2022**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2209.09513)[![github](https://img.shields.io/github/stars/lupantech/ScienceQA)](https://github.com/lupantech/ScienceQA)![alias](https://img.shields.io/badge/ScienceQA-black)
5. [Towards VQA Models That Can Read](https://arxiv.org/abs/1904.08920) (April. 18, 2019, **CVPR 2019**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1904.08920)[![github](https://img.shields.io/github/stars/facebookresearch/mmf)](https://github.com/facebookresearch/mmf)![alias](https://img.shields.io/badge/TextVQA-black)
6. [GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](https://arxiv.org/abs/1902.09506) (Feb. 25, 2019, **CVPR 2019**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1902.09506)![tag](https://img.shields.io/badge/Oral-FF4D00)![alias](https://img.shields.io/badge/GQA-black)
7. [Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering](https://arxiv.org/abs/1612.00837) (Dec. 2, 2016, **CVPR 2017**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1902.09506)![alias](https://img.shields.io/badge/VQAv2-black)



## :clap:MLLMs Acceleration

1. [VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/abs/2412.04467) (Dec. 5, 2024, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.04467)[![github](https://img.shields.io/github/stars/dvlab-research/VisionZip)](https://github.com/dvlab-research/VisionZip)![alias](https://img.shields.io/badge/VisionZip-black)
2. [CLS Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster](http://arxiv.org/abs/2412.01818) (Dec. 2, 2024) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.01818)[![github](https://img.shields.io/github/stars/Theia-4869/FasterVLM)](https://github.com/Theia-4869/FasterVLM)![alias](https://img.shields.io/badge/FasterVLM-black)
3. [SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/abs/2410.04417) (Oct. 6, 2024) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.04417)[![github](https://img.shields.io/github/stars/Gumpest/SparseVLMs)](https://github.com/Gumpest/SparseVLMs)![alias](https://img.shields.io/badge/SparseVLM-black)
4. [Matryoshka Query Transformer for Large Vision-Language Models](https://arxiv.org/abs/2405.19315) (May. 29, 2024, **NeurIPS 2**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.19315)[![github](https://img.shields.io/github/stars/gordonhu608/MQT-LLaVA)](https://github.com/gordonhu608/MQT-LLaVA)![alias](https://img.shields.io/badge/MQT-black)
5. [LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388) (Mar. 22, 2024)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.15388)[![github](https://img.shields.io/github/stars/42Shawn/LLaVA-PruMerge)](https://github.com/42Shawn/LLaVA-PruMerge)![alias](https://img.shields.io/badge/PruMerge-black)
6. [An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models](https://arxiv.org/abs/2403.06764) (Mar. 11, 2024, **ECCV 2024**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.06764)[![github](https://img.shields.io/github/stars/pkunlp-icler/FastV)](https://github.com/pkunlp-icler/FastV)![tag](https://img.shields.io/badge/Oral-FF4D00)![alias](https://img.shields.io/badge/FastV-black)
7. [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043) (Nov. 28, 2023)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.17043)[![github](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID)](https://github.com/dvlab-research/LLaMA-VID)![alias](https://img.shields.io/badge/LLaMAVID-black)

