# Awesome-MLLMs-Acceleration
This is a list of some awesome works on accelerating in Multimodal Large Language Models(MLLMs).



## :bar_chart:Benchmarks

1. [MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2306.13394) (Jun. 23, 2023) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.13394)[![github](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models)](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)![alias](https://img.shields.io/badge/MME-black)
2. [MMBench: Is Your Multi-modal Model an All-around Player?](https://arxiv.org/abs/2307.06281) (Jul. 12, 2023, **ECCV 2024**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2307.06281)[![github](https://img.shields.io/github/stars/open-compass/VLMEvalKit)](https://github.com/open-compass/VLMEvalKit)![tag](https://img.shields.io/badge/Oral-FF4D00)![alias](https://img.shields.io/badge/MMBench-black)
3. [Evaluating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2305.10355) (May. 17, 2023, **EMNLP 2023**) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.10355)[![github](https://img.shields.io/github/stars/AoiDragon/POPE)](https://github.com/AoiDragon/POPE)![alias](https://img.shields.io/badge/PoPE-black)
4. [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://arxiv.org/abs/2209.09513) (Sep. 20, 2022, **NeurIPS 2022**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2209.09513)[![github](https://img.shields.io/github/stars/lupantech/ScienceQA)](https://github.com/lupantech/ScienceQA)![alias](https://img.shields.io/badge/ScienceQA-black)
5. [Towards VQA Models That Can Read](https://arxiv.org/abs/1904.08920) (April. 18, 2019, **CVPR 2019**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1904.08920)[![github](https://img.shields.io/github/stars/facebookresearch/mmf)](https://github.com/facebookresearch/mmf)![alias](https://img.shields.io/badge/TextVQA-black)
6. [GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](https://arxiv.org/abs/1902.09506) (Feb. 25, 2019, **CVPR 2019**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1902.09506)![tag](https://img.shields.io/badge/Oral-FF4D00)![alias](https://img.shields.io/badge/GQA-black)
7. [Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering](https://arxiv.org/abs/1612.00837) (Dec. 2, 2016, **CVPR 2017**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1902.09506)![alias](https://img.shields.io/badge/VQAv2-black)



## :clap:MLLMs Acceleration

1. [Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models](https://arxiv.org/abs/2503.16036) (Mar. 20, 2025, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2503.16036)[![github](https://img.shields.io/github/stars/lntzm/HICom)](https://github.com/lntzm/HICom)![alias](https://img.shields.io/badge/HICom-black)
2. [EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models](https://arxiv.org/abs/2503.15369) (Mar. 19, 2025, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2503.15369)![alias](https://img.shields.io/badge/EfficientLLaVA-black)
3. [Adaptive Keyframe Sampling for Long Video Understanding](https://www.arxiv.org/abs/2502.21271) (Feb. 22, 2025, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2502.21271)[![github](https://img.shields.io/github/stars/ncTimTang/AKS)](https://github.com/ncTimTang/AKS)![alias](https://img.shields.io/badge/AKS-black)
4. [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://arxiv.org/abs/2501.03895) (Jan. 7, 2025, **ICLR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.03895)[![github](https://img.shields.io/github/stars/ictnlp/LLaVA-Mini)](https://github.com/ictnlp/LLaVA-Mini)![alias](https://img.shields.io/badge/LLaVAMini-black)
5. [FastVLM: Efficient Vision Encoding for Vision Language Models](https://arxiv.org/abs/2412.13303) (Dec. 17, 2024, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.13303)![alias](https://img.shields.io/badge/FastVLM-black)
6. [VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/abs/2412.04467) (Dec. 5, 2024, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.04467)[![github](https://img.shields.io/github/stars/dvlab-research/VisionZip)](https://github.com/dvlab-research/VisionZip)![alias](https://img.shields.io/badge/VisionZip-black)
7. [FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression](https://arxiv.org/abs/2412.04317) (Dec. 5, 2024, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.04317)[![github](https://img.shields.io/github/stars/codefanw/FlashSloth)](https://github.com/codefanw/FlashSloth)![alias](https://img.shields.io/badge/FlashSloth-black)
8. [CLS Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster](http://arxiv.org/abs/2412.01818) (Dec. 2, 2024) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.01818)[![github](https://img.shields.io/github/stars/Theia-4869/FasterVLM)](https://github.com/Theia-4869/FasterVLM)![alias](https://img.shields.io/badge/FasterVLM-black)
9. [DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models](https://arxiv.org/abs/2411.15024) (Nov. 22, 2024, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.15024)[![github](https://img.shields.io/github/stars/KD-TAO/DyCoke)](https://github.com/KD-TAO/DyCoke)![alias](https://img.shields.io/badge/DyCoke-black)
10. [SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/abs/2410.04417) (Oct. 6, 2024) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.04417)[![github](https://img.shields.io/github/stars/Gumpest/SparseVLMs)](https://github.com/Gumpest/SparseVLMs)![alias](https://img.shields.io/badge/SparseVLM-black)
11. [Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding](https://arxiv.org/abs/2409.14485) (Sep. 22, 2024, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2409.14485)[![github](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL)](https://github.com/VectorSpaceLab/Video-XL)![alias](https://img.shields.io/badge/Video XL-black)
12. [TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings](https://arxiv.org/abs/2409.09564) (Sep. 15, 2024, **AAAI 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2409.09564)[![github](https://img.shields.io/github/stars/AIDC-AI/TG-LLaVA)](https://github.com/AIDC-AI/TG-LLaVA)![alias](https://img.shields.io/badge/TGLLaVA-black)
13. [VoCo-LLaMA: Towards Vision Compression with Large Language Models](https://arxiv.org/abs/2406.12275v2) (Jun. 18, 2024, **CVPR 2025**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.12275v2)[![github](https://img.shields.io/github/stars/Yxxxb/VoCo-LLaMA)](https://github.com/Yxxxb/VoCo-LLaMA)![alias](https://img.shields.io/badge/VoCoLLaMA-black)
14. [Matryoshka Query Transformer for Large Vision-Language Models](https://arxiv.org/abs/2405.19315) (May. 29, 2024, **NeurIPS 2024**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.19315)[![github](https://img.shields.io/github/stars/gordonhu608/MQT-LLaVA)](https://github.com/gordonhu608/MQT-LLaVA)![alias](https://img.shields.io/badge/MQT-black)
15. [LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388) (Mar. 22, 2024)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.15388)[![github](https://img.shields.io/github/stars/42Shawn/LLaVA-PruMerge)](https://github.com/42Shawn/LLaVA-PruMerge)![alias](https://img.shields.io/badge/PruMerge-black)
16. [An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models](https://arxiv.org/abs/2403.06764) (Mar. 11, 2024, **ECCV 2024**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.06764)[![github](https://img.shields.io/github/stars/pkunlp-icler/FastV)](https://github.com/pkunlp-icler/FastV)![tag](https://img.shields.io/badge/Oral-FF4D00)![alias](https://img.shields.io/badge/FastV-black)
17. [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043) (Nov. 28, 2023, **ECCV 2024**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2311.17043)[![github](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID)](https://github.com/dvlab-research/LLaMA-VID)![alias](https://img.shields.io/badge/LLaMAVID-black)
18. [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453) (Sep. 29, 2023, **ICLR 2024**) [![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2309.17453)[![github](https://img.shields.io/github/stars/mit-han-lab/streaming-llm)](https://github.com/mit-han-lab/streaming-llm)![alias](https://img.shields.io/badge/StreamLLM-black)
19. [Token Merging: Your ViT But Faster](https://arxiv.org/abs/2210.09461v3) (Oct. 17, 2022, **ICLR 2023**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2210.09461v3)[![github](https://img.shields.io/github/stars/facebookresearch/ToMe)](https://github.com/facebookresearch/ToMe)![tag](https://img.shields.io/badge/Oral-FF4D00)![alias](https://img.shields.io/badge/ToMe-black)
20. [Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations](https://arxiv.org/abs/2202.07800) (Feb. 16, 2022, **ICLR 2022**)[![arxiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2202.07800)[![github](https://img.shields.io/github/stars/youweiliang/evit)](https://github.com/youweiliang/evit)![tag](https://img.shields.io/badge/Spotlight-FF4D00)![alias](https://img.shields.io/badge/EViT-black)

